{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "执行时间: 2.03 毫秒\n",
      "Attention输出： tensor([[[0.4623, 0.3159, 0.6992, 0.5253, 0.4834, 0.3035, 0.5636, 0.2056],\n",
      "         [0.4446, 0.3195, 0.6882, 0.5235, 0.5048, 0.2769, 0.5689, 0.2020],\n",
      "         [0.4413, 0.3142, 0.6860, 0.5176, 0.5019, 0.2756, 0.5704, 0.2002],\n",
      "         [0.4651, 0.3211, 0.6985, 0.5274, 0.4839, 0.3027, 0.5601, 0.2062]],\n",
      "\n",
      "        [[0.5355, 0.4025, 0.6384, 0.1917, 0.7205, 0.5478, 0.4214, 0.4450],\n",
      "         [0.5341, 0.4074, 0.6449, 0.1862, 0.7191, 0.5508, 0.4245, 0.4481],\n",
      "         [0.5426, 0.3713, 0.6287, 0.2080, 0.7285, 0.5407, 0.4197, 0.4284],\n",
      "         [0.5390, 0.3879, 0.6343, 0.1990, 0.7242, 0.5446, 0.4208, 0.4375]],\n",
      "\n",
      "        [[0.4563, 0.6375, 0.7427, 0.5676, 0.3553, 0.3468, 0.3884, 0.4641],\n",
      "         [0.4604, 0.6426, 0.7411, 0.5681, 0.3496, 0.3468, 0.3918, 0.4774],\n",
      "         [0.4909, 0.6675, 0.7252, 0.5581, 0.3440, 0.3570, 0.4080, 0.5080],\n",
      "         [0.4436, 0.6311, 0.7478, 0.5745, 0.3518, 0.3393, 0.3854, 0.4677]]])\n",
      "Attention权重： tensor([[[0.2313, 0.2421, 0.2860, 0.2407],\n",
      "         [0.2466, 0.2040, 0.2856, 0.2638],\n",
      "         [0.2499, 0.2020, 0.2934, 0.2548],\n",
      "         [0.2362, 0.2400, 0.2757, 0.2480]],\n",
      "\n",
      "        [[0.2273, 0.2337, 0.2949, 0.2440],\n",
      "         [0.2283, 0.2391, 0.2822, 0.2504],\n",
      "         [0.2619, 0.2114, 0.3250, 0.2017],\n",
      "         [0.2442, 0.2238, 0.3081, 0.2239]],\n",
      "\n",
      "        [[0.2682, 0.1948, 0.3037, 0.2332],\n",
      "         [0.2671, 0.1959, 0.2898, 0.2472],\n",
      "         [0.2753, 0.2227, 0.2415, 0.2605],\n",
      "         [0.2505, 0.1901, 0.3137, 0.2457]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "def attention(Q, K, V):\n",
    "\n",
    "    \"\"\"\n",
    "    参数：\n",
    "    - Q: 查询矩阵，形状为 (batch_size, seq_len_q, d_k)\n",
    "    - K: 键矩阵，形状为 (batch_size, seq_len_k, d_k)\n",
    "    - V: 值矩阵，形状为 (batch_size, seq_len_v, d_v)\n",
    "\n",
    "    返回：\n",
    "    - output: 注意力加权后的值矩阵，形状为 (batch_size, seq_len_q, d_v)\n",
    "    \"\"\"\n",
    "\n",
    "    # 计算注意力分数\n",
    "    scores = torch.matmul(Q, K.permute(0, 2, 1)) / torch.sqrt(torch.tensor(Q.size(-1)).float())\n",
    "    \n",
    "    # 使用softmax计算权重\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 对权重加权求和得到注意力结果\n",
    "    output = torch.matmul(weights, V)\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# 随机生成模拟数据\n",
    "torch.manual_seed(42)\n",
    "Q = torch.rand((3, 4, 5))  # 3个查询向量，每个查询向量有4个维度，总共有5个查询\n",
    "K = torch.rand((3, 4, 5))  # 3个键向量，每个键向量有4个维度，总共有5个键\n",
    "V = torch.rand((3, 4, 8))  # 3个数值向量，每个数值向量有4个维度，总共有8个数值\n",
    "\n",
    "\n",
    "# 创建 CPU 时间记录\n",
    "start_time = time.time()\n",
    "\n",
    "# 执行注意力机制函数\n",
    "output, weights = attention(Q, K, V)\n",
    "\n",
    "# 记录结束时间\n",
    "end_time = time.time()\n",
    "\n",
    "# 计算执行时间\n",
    "elapsed_time = (end_time - start_time) * 1000  # 转换为毫秒\n",
    "print(f\"执行时间: {elapsed_time:.2f} 毫秒\")\n",
    "\n",
    "\n",
    "# 打印结果\n",
    "print(\"Attention输出：\", output)\n",
    "print(\"Attention权重：\", weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
